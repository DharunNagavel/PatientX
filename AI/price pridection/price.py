# -*- coding: utf-8 -*-
"""price.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1RSobzZXCtX4jdKHHhBdNJ97UFffPx26B
"""

!pip install pdfplumber python-docx openpyxl pytesseract pandas Pillow

import os
import math
import re
import tempfile
import shutil
from datetime import datetime
import numpy as np
from PIL import Image
import pdfplumber
import pytesseract
import pandas as pd
from docx import Document
from sklearn.ensemble import RandomForestRegressor
from sklearn.preprocessing import StandardScaler
import joblib

# ---------- Constants ----------
BASE_PRICE = 200.0
HOSPITAL_CATEGORIES = {"primary": 1.0, "secondary": 1.2, "high-level": 1.5, "superspeciality": 1.8}
MIN_FILES = 1
MAX_FILES = 5

REPORT_TYPE_MAPPING = {
    "x-ray": ["x-ray", "xray", "radiograph", "chest x-ray"],
    "scan": ["ct", "mri", "ultrasound", "scan", "tomography", "angiogram"],
    "tabular": [".csv", ".xls", ".xlsx", "laboratory", "lab report"],
    "medical_report": ["diagnosis", "medical", "report", ".pdf", ".docx", ".doc", ".txt", "discharge", "clinical"],
    "surgery_report": ["surgery", "operation", "procedure", "operative"],
    "pathology": ["biopsy", "pathology", "histopathology", "cytology"],
    "emergency": ["emergency", "trauma", "critical", "icu"]
}

# Medical specialty multipliers
SPECIALTY_MULTIPLIERS = {
    "cardiology": 1.4,
    "neurology": 1.5,
    "oncology": 1.6,
    "orthopedics": 1.2,
    "pediatrics": 1.1,
    "radiology": 1.3,
    "surgery": 1.4,
    "general": 1.0
}

# ---------- Machine Learning Model ----------
class PricingModel:
    def __init__(self):
        self.model = RandomForestRegressor(n_estimators=100, random_state=42)
        self.scaler = StandardScaler()
        self.is_trained = False

    def prepare_features(self, features_dict):
        """Convert features dictionary to numpy array"""
        feature_order = [
            'pages', 'quality_score', 'hospital_mult', 'report_type_mult',
            'urgency_mult', 'specialty_mult', 'complexity_score', 'word_count',
            'image_count', 'tables_count', 'medical_terms_density'
        ]
        return np.array([[features_dict[feature] for feature in feature_order]])

    def train(self, X, y):
        """Train the model with historical data"""
        if len(X) > 0:
            X_scaled = self.scaler.fit_transform(X)
            self.model.fit(X_scaled, y)
            self.is_trained = True

    def predict(self, features_dict):
        """Predict price using trained model"""
        if not self.is_trained:
            return self._fallback_prediction(features_dict)

        X = self.prepare_features(features_dict)
        X_scaled = self.scaler.transform(X)
        return max(BASE_PRICE * 0.5, self.model.predict(X_scaled)[0])

    def _fallback_prediction(self, features_dict):
        """Fallback to rule-based pricing if model not trained"""
        base = BASE_PRICE
        adjusted = (base * features_dict['hospital_mult'] *
                   features_dict['report_type_mult'] *
                   features_dict['specialty_mult'] *
                   features_dict['urgency_mult'])
        quality_bonus = 1 + (features_dict['quality_score'] * 0.5)
        complexity_bonus = 1 + (features_dict['complexity_score'] * 0.3)
        return max(BASE_PRICE * 0.5, adjusted * quality_bonus * complexity_bonus)

# ---------- Enhanced Helper Functions ----------
def _save_to_tmp_from_path(filepath):
    tmp = tempfile.NamedTemporaryFile(delete=False, suffix=os.path.splitext(filepath)[1])
    tmp_path = tmp.name
    shutil.copy(filepath, tmp_path)
    return tmp_path

def extract_text_and_metadata(file_path):
    ext = os.path.splitext(file_path)[1].lower()
    text = ""
    pages = 1
    word_count = 0
    image_count = 0
    tables_count = 0

    try:
        if ext == ".pdf":
            with pdfplumber.open(file_path) as pdf:
                pages = len(pdf.pages)
                for page in pdf.pages:
                    text += page.extract_text() or ""
                    # Count images in PDF
                    image_count += len(page.images) if hasattr(page, 'images') else 0
                    # Count tables in PDF
                    tables = page.find_tables()
                    tables_count += len(tables) if tables else 0

        elif ext in [".docx", ".doc"]:
            doc = Document(file_path)
            paragraphs = [p.text for p in doc.paragraphs if p.text.strip()]
            pages = max(1, len(paragraphs)//20 + 1)
            text = "\n".join(paragraphs)
            # Estimate images in DOCX (simplified)
            image_count = len([p for p in doc.paragraphs if p._element.xpath('.//pic:pic')])

        elif ext == ".txt":
            with open(file_path, "r", encoding="utf-8", errors="ignore") as f:
                text = f.read()
                pages = max(1, len(text.splitlines())//40)

        elif ext in [".jpg", ".jpeg", ".png", ".tiff", ".bmp"]:
            img = Image.open(file_path)
            text = pytesseract.image_to_string(img)
            pages = 1
            image_count = 1

        elif ext == ".csv":
            df = pd.read_csv(file_path, dtype=str)
            text = "\n".join(df.fillna("").astype(str).apply(lambda row: " | ".join(row.values), axis=1).tolist())
            pages = max(1, len(df)//40 + 1)
            tables_count = 1

        elif ext in [".xls", ".xlsx"]:
            xls = pd.read_excel(file_path, sheet_name=None, dtype=str)
            parts = []
            total_rows = 0
            for sheetname, df in xls.items():
                df = df.fillna("").astype(str)
                total_rows += len(df)
                if len(df):
                    parts.append("\n".join(df.apply(lambda row: " | ".join(row.values), axis=1).tolist()))
            text = "\n\n".join(parts)
            pages = max(1, total_rows//40 + 1)
            tables_count = len(xls)

        else:
            with open(file_path, "rb") as f:
                raw = f.read()
            try:
                text = raw.decode("utf-8", errors="ignore")
            except:
                text = ""
            pages = max(1, len(text.splitlines())//40 if text else 1)

        word_count = len(text.split())

    except Exception as e:
        print(f"Error extracting text from {file_path}: {e}")

    return text, pages, word_count, image_count, tables_count

def detect_report_type(file_path, text):
    ext = os.path.splitext(file_path)[1].lower()
    t = text.lower()
    file_name = os.path.basename(file_path).lower()

    # Check file name and content for report type indicators
    for rtype, keywords in REPORT_TYPE_MAPPING.items():
        for kw in keywords:
            if (kw in ext or kw in t or kw in file_name):
                return rtype
    return "medical_report"

def detect_medical_specialty(text):
    """Detect medical specialty from text content"""
    t = text.lower()
    specialty_keywords = {
        "cardiology": ["cardiac", "ecg", "echo", "heart", "coronary", "angiogram"],
        "neurology": ["neuro", "brain", "mri brain", "eeg", "seizure", "stroke"],
        "oncology": ["cancer", "tumor", "chemotherapy", "oncology", "malignant"],
        "orthopedics": ["fracture", "bone", "ortho", "joint", "spine", "dislocation"],
        "pediatrics": ["pediatric", "child", "infant", "neonatal", "growth chart"],
        "radiology": ["radiology", "x-ray", "ct", "mri", "ultrasound", "imaging"],
        "surgery": ["surgery", "operative", "post-op", "resection", "anesthesia"]
    }

    for specialty, keywords in specialty_keywords.items():
        if any(keyword in t for keyword in keywords):
            return specialty
    return "general"

def enhanced_quality_score(text, word_count, image_count, tables_count):
    """Enhanced quality scoring with multiple factors"""
    t = text.lower()
    score = 0

    # Content quality checks
    checks = {
        "diagnosis": bool(re.search(r"diagnosis|impression|conclusion", t)),
        "findings": bool(re.search(r"findings|observations|results", t)),
        "imaging_terms": bool(re.search(r"ct|mri|x[- ]?ray|ultrasound|radiograph", t)),
        "clinical_values": bool(re.search(r"\d+\s*(mg|mmhg|cm|mm|%|mg/dl|mmol/l)", t)),
        "medical_professional": bool(re.search(r"dr\.|doctor|physician|consultant|signature", t)),
        "patient_info": bool(re.search(r"patient|age|sex|gender|dob|history", t)),
        "medications": bool(re.search(r"medication|prescription|drug|dose", t)),
        "procedures": bool(re.search(r"procedure|technique|methodology", t))
    }

    weights = {
        "diagnosis": 0.2, "findings": 0.15, "imaging_terms": 0.1,
        "clinical_values": 0.1, "medical_professional": 0.1,
        "patient_info": 0.1, "medications": 0.125, "procedures": 0.125
    }

    for k, present in checks.items():
        if present:
            score += weights[k]

    # Structure bonuses
    if word_count > 500:
        score += 0.1
    if image_count > 0:
        score += 0.05
    if tables_count > 0:
        score += 0.05

    return min(1.0, score)

def calculate_complexity_score(text, pages, word_count, image_count, tables_count):
    """Calculate complexity score based on multiple factors"""
    complexity = 0

    # Page complexity
    page_complexity = min(pages / 10, 1.0) * 0.3

    # Content density complexity
    word_density = min(word_count / 1000, 1.0) * 0.3

    # Structural complexity
    structural = min((image_count * 0.1 + tables_count * 0.2), 0.4) * 0.4

    complexity = page_complexity + word_density + structural
    return min(1.0, complexity)

def detect_urgency(text):
    """Detect urgency level from text content"""
    t = text.lower()
    urgent_terms = ["emergency", "stat", "urgent", "critical", "asap", "immediate", "trauma"]
    if any(term in t for term in urgent_terms):
        return 1.3  # 30% premium for urgent reports
    return 1.0

def medical_terms_density(text):
    """Calculate medical terminology density"""
    medical_terms = [
        "diagnosis", "prognosis", "etiology", "symptom", "syndrome",
        "treatment", "therapy", "medication", "dose", "procedure",
        "findings", "observations", "clinical", "pathology"
    ]
    words = text.lower().split()
    if not words:
        return 0
    medical_count = sum(1 for word in words if any(term in word for term in medical_terms))
    return medical_count / len(words)

def compute_enhanced_price(features, pricing_model, num_reports):
    """Compute price using ML model or fallback to rules"""
    # Apply bulk discount
    bulk_discount = 0.90 if num_reports >= 3 else (0.95 if num_reports > 1 else 1.0)

    # Get base prediction
    predicted_price = pricing_model.predict(features)

    # Apply bulk discount
    final_price = predicted_price * bulk_discount

    return round(max(BASE_PRICE * 0.5, final_price), 2)

# ---------- Main Function ----------
def main():
    print(f"Please enter the paths to 1â€“5 medical report files, separated by commas:")
    paths_input = input("Paths: ").strip()
    files = [p.strip() for p in paths_input.split(",") if os.path.isfile(p.strip())]

    # Duplicate check
    if len(files) != len(set(files)):
        print("Error: You have uploaded the same file more than once. Please submit unique files only.")
        return

    if len(files) < MIN_FILES or len(files) > MAX_FILES:
        print(f"You selected {len(files)} files. Please select between 1 and 5 unique files. Exiting.")
        return

    # Get hospital category
    hospital_category = input("Enter hospital category (primary/secondary/high-level/superspeciality): ").strip().lower()
    if hospital_category not in HOSPITAL_CATEGORIES:
        print("Invalid hospital category. Defaulting to 'primary'.")
        hospital_category = "primary"
    hospital_mult = HOSPITAL_CATEGORIES[hospital_category]
    print(f"Hospital multiplier applied: {hospital_mult}")

    # Initialize pricing model
    pricing_model = PricingModel()

    # In a real application, you would load pre-trained model here
    # pricing_model = joblib.load('medical_pricing_model.pkl')

    total_price = 0
    summary = []

    print("\nAnalyzing reports...")
    for i, file_path in enumerate(files, 1):
        tmp_path = _save_to_tmp_from_path(file_path)
        try:
            text, pages, word_count, image_count, tables_count = extract_text_and_metadata(tmp_path)
            report_type = detect_report_type(file_path, text)
            specialty = detect_medical_specialty(text)
            urgency_mult = detect_urgency(text)

            # Calculate scores
            quality = enhanced_quality_score(text, word_count, image_count, tables_count)
            complexity = calculate_complexity_score(text, pages, word_count, image_count, tables_count)
            med_terms_density = medical_terms_density(text)

            # Prepare features for pricing
            features = {
                'pages': pages,
                'quality_score': quality,
                'hospital_mult': hospital_mult,
                'report_type_mult': 1.2 if report_type in ["scan", "surgery_report"] else 1.0,
                'urgency_mult': urgency_mult,
                'specialty_mult': SPECIALTY_MULTIPLIERS.get(specialty, 1.0),
                'complexity_score': complexity,
                'word_count': min(word_count / 100, 10),  # Normalize
                'image_count': image_count,
                'tables_count': tables_count,
                'medical_terms_density': med_terms_density
            }

            price = compute_enhanced_price(features, pricing_model, len(files))
            total_price += price

            summary.append({
                "file": os.path.basename(file_path),
                "report_type": report_type,
                "specialty": specialty,
                "pages": pages,
                "word_count": word_count,
                "images": image_count,
                "tables": tables_count,
                "quality": round(quality, 2),
                "complexity": round(complexity, 2),
                "urgency": "High" if urgency_mult > 1.0 else "Normal",
                "price": price
            })

            print(f"âœ“ Analyzed {os.path.basename(file_path)}")

        except Exception as e:
            print(f"âœ— Error processing {os.path.basename(file_path)}: {e}")
        finally:
            try:
                os.remove(tmp_path)
            except:
                pass

    # Display detailed summary
    print("\n" + "="*80)
    print("REPORT ANALYSIS SUMMARY")
    print("="*80)

    for i, rec in enumerate(summary, 1):
        print(f"\n[{i}] {rec['file']}")
        print(f"    Type: {rec['report_type'].upper()} | Specialty: {rec['specialty'].upper()}")
        print(f"    Pages: {rec['pages']} | Words: {rec['word_count']} | Images: {rec['images']} | Tables: {rec['tables']}")
        print(f"    Quality: {rec['quality']} | Complexity: {rec['complexity']} | Urgency: {rec['urgency']}")
        print(f"    Suggested Price: â‚¹{rec['price']}")

    print("\n" + "="*80)
    print(f"TOTAL SUGGESTED PRICE for {len(summary)} reports: â‚¹{total_price:.2f}")

    # Additional insights
    avg_quality = np.mean([rec['quality'] for rec in summary])
    avg_complexity = np.mean([rec['complexity'] for rec in summary])

    print(f"\nAVERAGE QUALITY SCORE: {avg_quality:.2f}/1.0")
    print(f"AVERAGE COMPLEXITY SCORE: {avg_complexity:.2f}/1.0")

    if avg_quality > 0.7:
        print("ðŸ“Š Insight: High quality reports - good documentation")
    elif avg_quality < 0.4:
        print("ðŸ“Š Insight: Low quality reports - may need additional information")

    if total_price > BASE_PRICE * len(summary) * 2:
        print("ðŸ’° Insight: Premium pricing due to complexity and quality factors")

    print("="*80)

if __name__ == "__main__":
    main()